{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_DT_Instructions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jKMeoHjWN-3k"},"source":["# <b>Assignment : DT</b>"]},{"cell_type":"markdown","metadata":{"id":"w0vpJMx2pHYX"},"source":["<font color='red'><b> Please check below video before attempting this assignment</b></font>"]},{"cell_type":"code","metadata":{"id":"IrJVk4Chpzjp"},"source":["from IPython.display import YouTubeVideo\n","YouTubeVideo('ZhLXULFjIjQ', width=\"1000\",height=\"500\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjA-ZU-TqVK1"},"source":["<font color='red'><b> TF-IDFW2V</b></font>"]},{"cell_type":"markdown","metadata":{"id":"uvBr2z6iqW9V"},"source":["<b>Tfidf w2v (w1,w2..) = (tfidf(w1) * w2v(w1) + tfidf(w2) * w2v(w2) + …)  /    (tfidf(w1) + tfidf(w2) + …)</b>"]},{"cell_type":"markdown","metadata":{"id":"zRAy5UzOvi_a"},"source":["<b>(Optional) Please check course video on [AVgw2V and TF-IDFW2V ](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2916/avg-word2vec-tf-idf-weighted-word2vec/3/module-3-foundations-of-natural-language-processing-and-machine-learning)for more details."]},{"cell_type":"markdown","metadata":{"id":"IB2uk5LwtBlO"},"source":["<font color='blue'><b>Glove vectors </b></font>"]},{"cell_type":"markdown","metadata":{"id":"j697XLZGtCnz"},"source":["<b>In this assignment you will be working with glove vectors , please check  [this](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) and [this](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) for more details.</b><br>\n","\n","Download glove vectors from this [link ](https://drive.google.com/file/d/1lDca_ge-GYO0iQ6_XDLWePQFMdAA2b8f/view?usp=sharing)"]},{"cell_type":"code","metadata":{"id":"_ufHLoACuoHw"},"source":["#please use below code to load glove vectors \n","with open('glove_vectors', 'rb') as f:\n","    model = pickle.load(f)\n","    glove_words =  set(model.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YMurhntCAooj"},"source":["or else , you can use below code"]},{"cell_type":"code","metadata":{"id":"8-KyNeiQAtLG"},"source":["'''\n","# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n","def loadGloveModel(gloveFile):\n","    print (\"Loading Glove Model\")\n","    f = open(gloveFile,'r', encoding=\"utf8\")\n","    model = {}\n","    for line in tqdm(f):\n","        splitLine = line.split()\n","        word = splitLine[0]\n","        embedding = np.array([float(val) for val in splitLine[1:]])\n","        model[word] = embedding\n","    print (\"Done.\",len(model),\" words loaded!\")\n","    return model\n","model = loadGloveModel('glove.42B.300d.txt')\n","\n","# ============================\n","Output:\n","    \n","Loading Glove Model\n","1917495it [06:32, 4879.69it/s]\n","Done. 1917495  words loaded!\n","\n","# ============================\n","\n","words = []\n","for i in preproced_texts:\n","    words.extend(i.split(' '))\n","\n","for i in preproced_titles:\n","    words.extend(i.split(' '))\n","print(\"all the words in the coupus\", len(words))\n","words = set(words)\n","print(\"the unique words in the coupus\", len(words))\n","\n","inter_words = set(model.keys()).intersection(words)\n","print(\"The number of words that are present in both glove vectors and our coupus\", \\\n","      len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\n","\n","words_courpus = {}\n","words_glove = set(model.keys())\n","for i in words:\n","    if i in words_glove:\n","        words_courpus[i] = model[i]\n","print(\"word 2 vec length\", len(words_courpus))\n","\n","\n","# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n","\n","import pickle\n","with open('glove_vectors', 'wb') as f:\n","    pickle.dump(words_courpus, f)\n","\n","\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OTJ7Et5hxpZS"},"source":["# <font color='red'> <b>Task - 1</b></font>"]},{"cell_type":"markdown","metadata":{"id":"ACUkHex3N-3m"},"source":["<ol>\n","    <li><strong>Apply Decision Tree Classifier(DecisionTreeClassifier) on these feature sets</strong>\n","        <ul>\n","            <li><font color='red'>Set 1</font>: categorical, numerical features +  preprocessed_essay (TFIDF) + Sentiment scores(preprocessed_essay)</li>\n","            <li><font color='red'>Set 2</font>: categorical, numerical features +  preprocessed_essay (TFIDF W2V) + Sentiment scores(preprocessed_essay)</li>        </ul>\n","    </li>\n","    <li><strong>The hyper paramter tuning (best `depth` in range [1, 3, 10, 30], and the best `min_samples_split` in range [5, 10, 100, 500])</strong>\n","        <ul>\n","    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n","    <li>find the best hyper paramter using k-fold cross validation(use gridsearch cv or randomsearch cv)/simple cross validation data(you can write your own for loops refer sample solution)</li>\n","        </ul>\n","    </li>\n","    <li>\n","    <strong>Representation of results</strong>\n","        <ul>\n","    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n","    <img src='https://i.imgur.com/Gp2DQmh.jpg' width=500px> with X-axis as <strong>min_sample_split</strong>, Y-axis as <strong>max_depth</strong>, and Z-axis as <strong>AUC Score</strong> , we have given the notebook which explains how to plot this 3d plot, you can find it in the same drive <i>3d_scatter_plot.ipynb</i></li>\n","            <p style=\"text-align:center;font-size:30px;color:red;\"><strong>or</strong></p> <br>\n","    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n","    <img src='https://i.imgur.com/fgN9aUP.jpg' width=300px> <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heat maps</a> with rows as <strong>min_sample_split</strong>, columns as <strong>max_depth</strong>, and values inside the cell representing <strong>AUC Score</strong> </li>\n","    <li>You choose either of the plotting techniques out of 3d plot or heat map</li>\n","    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n","        Make sure that you are using predict_proba method to calculate AUC curves, because AUC is calcualted on class probabilities and not on class labels.\n","    <img src='https://i.imgur.com/wMQDTFe.jpg' width=300px></li>\n","    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points\n","    <img src='https://i.imgur.com/IdN5Ctv.png' width=300px></li>\n","    <li>Once after you plot the confusion matrix with the test data, get all the `false positive data points`\n","        <ul>\n","            <li> Plot the WordCloud(https://www.geeksforgeeks.org/generating-word-cloud-python/) with the words of essay text of these `false positive data points`</li>\n","            <li> Plot the box plot with the `price` of these `false positive data points`</li>\n","            <li> Plot the pdf with the `teacher_number_of_previously_posted_projects` of these `false positive data points`</li>\n","        </ul>\n","        </ul>\n","    </li>\n","   "]},{"cell_type":"code","metadata":{"id":"MmNrc-8piqK-"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aqWyfo1Sx8ua"},"source":["# <font color='red'><b> Task - 2 </b></font>"]},{"cell_type":"markdown","metadata":{"id":"Xddr3kChx-ew"},"source":["For this task consider **set-1** features.\n","\n","*  Select all the features which are having non-zero feature importance.You can get the feature importance using  'feature_importances_` \n","   (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), discard the all other remaining features and then apply any of the model of you choice i.e. (Dession tree, Logistic Regression, Linear SVM).\n","*  You need to do hyperparameter tuning corresponding to the model you selected and procedure in step 2 and step 3<br>\n","  **Note**: when you want to find the feature importance make sure you don't use max_depth parameter keep it None.\n","  </li>\n","    <br>\n","You need to summarize the results at the end of the notebook, summarize it in the table format\n","        <img src='http://i.imgur.com/YVpIGGE.jpg' width=400px>\n","    </li>\n","</ol>"]},{"cell_type":"markdown","metadata":{"id":"oZ-qDp6KxNj0"},"source":["<font color='blue'><b>Hint for calculating Sentiment scores</b></font>"]},{"cell_type":"code","metadata":{"id":"2IHTExN4xd5p"},"source":["# import nltk\n","# nltk.download('vader_lexicon')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"YKZIvFYBxaaD","outputId":"d9dcffbf-971d-4220-c03e-bf01cd81bdd4"},"source":["import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","# import nltk\n","# nltk.download('vader_lexicon')\n","\n","sid = SentimentIntensityAnalyzer()\n","\n","sample_sentence_1='I am happy.'\n","ss_1 = sid.polarity_scores(sample_sentence_1)\n","print('sentiment score for sentence 1',ss_1)\n","\n","sample_sentence_2='I am sad.'\n","ss_2 = sid.polarity_scores(sample_sentence_2)\n","print('sentiment score for sentence 2',ss_2)\n","\n","sample_sentence_3='I am going to New Delhi tommorow.'\n","ss_3 = sid.polarity_scores(sample_sentence_3)\n","print('sentiment score for sentence 3',ss_3)\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["sentiment score for sentence 1 {'neg': 0.0, 'neu': 0.213, 'pos': 0.787, 'compound': 0.5719}\n","sentiment score for sentence 2 {'neg': 0.756, 'neu': 0.244, 'pos': 0.0, 'compound': -0.4767}\n","sentiment score for sentence 3 {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"]}]},{"cell_type":"markdown","metadata":{"id":"r6FUMj5TN-3y"},"source":["<h1> Decision Tree </h1>"]},{"cell_type":"markdown","metadata":{"id":"luEzcFrGiqLa"},"source":["# <font color='red'> <b>Task - 1</b></font>"]},{"cell_type":"markdown","metadata":{"id":"QQmid3VAN-31"},"source":["## 1.1 Loading Data"]},{"cell_type":"code","metadata":{"id":"NqY4ES_3N-33"},"source":["#make sure you are loading atleast 50k datapoints\n","#you can work with features of preprocessed_data.csv for the assignment.\n","import pandas\n","data = pandas.read_csv('preprocessed_data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cT7V9hz7iqLm"},"source":["# write your code in following steps for task 1\n","# 1. calculate sentiment scores for the essay feature \n","# 2. Split your data.\n","# 3. perform tfidf vectorization of text data.\n","# 4. perform tfidf w2v vectorization of text data.\n","# 5. perform encoding of categorical features.\n","# 6. perform encoding of numerical features\n","# 7. For task 1 set 1 stack up all the features\n","# 8. For task 1 set 2 stack up all the features (for stacking dense features you can use np.stack)\n","# 9. Perform hyperparameter tuning and plot either heatmap or 3d plot.\n","# 10. Find the best parameters and fit the model. Plot ROC-AUC curve(using predict proba method)\n","# 11. Plot confusion matrix based on best threshold value\n","# 12. Find all the false positive data points and plot wordcloud of essay text and pdf of teacher_number_of_previously_posted_projects.\n","# 13. Write your observations about the wordcloud and pdf."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHvBlI_9N-4X"},"source":["# please write all the code with proper documentation, and proper titles for each subsection\n","# go through documentations and blogs before you start coding\n","# first figure out what to do, and then think about how to do.\n","# reading and understanding error messages will be very much helpfull in debugging your code\n","# when you plot any graph make sure you use \n","    # a. Title, that describes your plot, this will be very helpful to the reader\n","    # b. Legends if needed\n","    # c. X-axis label\n","    # d. Y-axis label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCT9iOq7iqLu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MeSKca6viqLw"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XmHih9-YiqLx"},"source":["# <font color='red'> <b>Task - 2</b></font>"]},{"cell_type":"code","metadata":{"id":"zk2-az7QiqLz"},"source":["# 1. write your code in following steps for task 2\n","# 2. select all non zero features\n","# 3. Update your dataset i.e. X_train,X_test and X_cv so that it contains all rows and only non zero features\n","# 4. perform hyperparameter tuning and plot either heatmap or 3d plot.\n","# 5. Fit the best model. Plot ROC AUC curve and confusion matrix similar to model 1.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N44iBzrmiqL1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WRDi4wd8iqL2"},"source":["# Tabulate your results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fSVKa3RviqL3"},"source":[""],"execution_count":null,"outputs":[]}]}